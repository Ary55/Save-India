# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkw50plgn9Yn9iRysWJXtZojAkiNdOUr
"""

from nltk.corpus import stopwords    
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
stop_words = set(stopwords.words('english'))

from nltk.tag import pos_tag
from nltk import word_tokenize
from collections import Counter

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go


from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix  
from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
import sklearn.linear_model
import sklearn.neighbors

df = pd.read_csv("corona_fake.csv")
#takes in file and this is what we test algo on and where we draw featues from
display(df)

pd.set_option('display.max_columns', 500)

df = pd.read_csv('corona_fake.csv')
df.loc[df['label'] == 'Fake', ['label']] = 'FAKE'
df.loc[df['label'] == 'fake', ['label']] = 'FAKE'
df.loc[df['source'] == 'facebook', ['source']] = 'Facebook'
df.text.fillna(df.title, inplace=True)

df.loc[5]['label'] = 'FAKE'
df.loc[15]['label'] = 'TRUE'
df.loc[43]['label'] = 'FAKE'
df.loc[131]['label'] = 'TRUE'
df.loc[242]['label'] = 'FAKE'

df = df.sample(frac=1).reset_index(drop=True)
df.title.fillna('missing', inplace=True)
df.source.fillna('missing', inplace=True)

df['title_num_uppercase'] = df['title'].str.count(r'[A-Z]')
df['text_num_uppercase'] = df['text'].str.count(r'[A-Z]')
df['text_len'] = df['text'].str.len()
df['text_pct_uppercase'] = df.text_num_uppercase.div(df.text_len)

df['title_num_stop_words'] = df['title'].str.split().apply(lambda x: len(set(x) & stop_words))
df['text_num_stop_words'] = df['text'].str.split().apply(lambda x: len(set(x) & stop_words))
df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))
df['text_pct_stop_words'] = df['text_num_stop_words'] / df['text_word_count']

"""FEATURES FOR TITLE

"""

df['title_num_stop_words'] = df['title'].str.split().apply(lambda x: len(set(x) & stop_words))
df['text_num_stop_words'] = df['text'].str.split().apply(lambda x: len(set(x) & stop_words))
df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))
df['text_pct_stop_words'] = df['text_num_stop_words'] / df['text_word_count']

df['title_num_uppercase'] = df['title'].str.count(r'[A-Z]')
df['text_num_uppercase'] = df['text'].str.count(r'[A-Z]')
df['text_len'] = df['text'].str.len()
df['text_pct_uppercase'] = df.text_num_uppercase.div(df.text_len)

df.drop(['text_num_uppercase', 'text_len', 'text_num_stop_words', 'text_word_count'], axis=1, inplace=True)
df['token'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)
df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1)
tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list())
df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1)
df = df[['title', 'text', 'source', 'label', 'title_num_uppercase', 'text_pct_uppercase', 'title_num_stop_words', 'text_pct_stop_words', 'NNP']].rename(columns={'NNP': 'NNP_title'})

df['token'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)
df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1)
tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list())
df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1)

df['num_negation'] = df['text'].str.lower().str.count("no|not|never|none|nothing|nobody|neither|nowhere|hardly|scarcely|barely|doesn’t|isn’t|wasn’t|shouldn’t|wouldn’t|couldn’t|won’t|can't|don't")
df['num_interrogatives_title'] = df['title'].str.lower().str.count("what|who|when|where|which|why|how")
df['num_interrogatives_text'] = df['text'].str.lower().str.count("what|who|when|where|which|why|how")

df['num_powerWords_text'] = df['text'].str.lower().str.count('improve|trust|immediately|discover|profit|learn|know|understand|powerful|best|win|more|bonus|exclusive|extra|you|free|health|guarantee|new|proven|safety|money|now|today|results|protect|help|easy|amazing|latest|extraordinary|how to|worst|ultimate|hot|first|big|anniversary|premiere|basic|complete|save|plus|create')
df['num_casualWords_text'] = df['text'].str.lower().str.count('make|because|how|why|change|use|since|reason|therefore|result')
df['num_tentativeWords_text'] = df['text'].str.lower().str.count('may|might|can|could|possibly|probably|it is likely|it is unlikely|it is possible|it is probable|tends to|appears to|suggests that|seems to')
df['num_emotionWords_text'] = df['text'].str.lower().str.count('ordeal|outrageous|provoke|repulsive|scandal|severe|shameful|shocking|terrible|tragic|unreliable|unstable|wicked|aggravate|agony|appalled|atrocious|corruption|damage|disastrous|disgusted|dreadful|eliminate|harmful|harsh|inconsiderate|enraged|offensive|aggressive|frustrated|controlling|resentful|anger|sad|fear|malicious|infuriated|critical|violent|vindictive|furious|contrary|condemning|sarcastic|poisonous|jealous|retaliating|desperate|alienated|unjustified|violated')

def new_data(df):
  df['title_num_uppercase'] = df['title'].str.count(r'[A-Z]')
  df['text_num_uppercase'] = df['text'].str.count(r'[A-Z]')
  df['text_len'] = df['text'].str.len()
  df['text_pct_uppercase'] = df.text_num_uppercase.div(df.text_len)

  df['title_num_stop_words'] = df['title'].str.split().apply(lambda x: len(set(x) & stop_words))
  df['text_num_stop_words'] = df['text'].str.split().apply(lambda x: len(set(x) & stop_words))
  df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))
  df['text_pct_stop_words'] = df['text_num_stop_words'] / df['text_word_count']

  df['title_num_stop_words'] = df['title'].str.split().apply(lambda x: len(set(x) & stop_words))
  df['text_num_stop_words'] = df['text'].str.split().apply(lambda x: len(set(x) & stop_words))
  df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))
  df['text_pct_stop_words'] = df['text_num_stop_words'] / df['text_word_count']

  df['title_num_uppercase'] = df['title'].str.count(r'[A-Z]')
  df['text_num_uppercase'] = df['text'].str.count(r'[A-Z]')
  df['text_len'] = df['text'].str.len()
  df['text_pct_uppercase'] = df.text_num_uppercase.div(df.text_len)

  df.drop(['text_num_uppercase', 'text_len', 'text_num_stop_words', 'text_word_count'], axis=1, inplace=True)
  df['token'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)
  df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1)
  tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list())
  df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1)
  df = df[['title', 'text', 'source', 'label', 'title_num_uppercase', 'text_pct_uppercase', 'title_num_stop_words', 'text_pct_stop_words', 'NNP']].rename(columns={'NNP': 'NNP_title'})

  df['token'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)
  df['pos_tags'] = df.apply(lambda row: nltk.pos_tag(row['token']), axis=1)
  tag_count_df = pd.DataFrame(df['pos_tags'].map(lambda x: Counter(tag[1] for tag in x)).to_list())
  df = pd.concat([df, tag_count_df], axis=1).fillna(0).drop(['pos_tags', 'token'], axis=1)

  df['num_negation'] = df['text'].str.lower().str.count("no|not|never|none|nothing|nobody|neither|nowhere|hardly|scarcely|barely|doesn’t|isn’t|wasn’t|shouldn’t|wouldn’t|couldn’t|won’t|can't|don't")
  df['num_interrogatives_title'] = df['title'].str.lower().str.count("what|who|when|where|which|why|how")
  df['num_interrogatives_text'] = df['text'].str.lower().str.count("what|who|when|where|which|why|how")

  df['num_powerWords_text'] = df['text'].str.lower().str.count('improve|trust|immediately|discover|profit|learn|know|understand|powerful|best|win|more|bonus|exclusive|extra|you|free|health|guarantee|new|proven|safety|money|now|today|results|protect|help|easy|amazing|latest|extraordinary|how to|worst|ultimate|hot|first|big|anniversary|premiere|basic|complete|save|plus|create')
  df['num_casualWords_text'] = df['text'].str.lower().str.count('make|because|how|why|change|use|since|reason|therefore|result')
  df['num_tentativeWords_text'] = df['text'].str.lower().str.count('may|might|can|could|possibly|probably|it is likely|it is unlikely|it is possible|it is probable|tends to|appears to|suggests that|seems to')
  df['num_emotionWords_text'] = df['text'].str.lower().str.count('ordeal|outrageous|provoke|repulsive|scandal|severe|shameful|shocking|terrible|tragic|unreliable|unstable|wicked|aggravate|agony|appalled|atrocious|corruption|damage|disastrous|disgusted|dreadful|eliminate|harmful|harsh|inconsiderate|enraged|offensive|aggressive|frustrated|controlling|resentful|anger|sad|fear|malicious|infuriated|critical|violent|vindictive|furious|contrary|condemning|sarcastic|poisonous|jealous|retaliating|desperate|alienated|unjustified|violated')

  dfnew = df
  return dfnew

X, y = df.drop(['title', 'text', 'source', 'label'], axis = 1), df['label']
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)

model = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 8)
model.fit(X, y)


svc=LinearSVC(dual=False)
scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')


num = np.array([["COVID research: a year of scientific milestones", "A single dose of the COVID-19 vaccine made by either Pfizer or AstraZeneca cuts a person’s risk of transmitting SARS-CoV-2 to their closest contacts by as much as half, according to an analysis of more than 365,000 households in the United Kingdom.", "Facebook.com", "missing"],["COVID research: a year of scientific milestones", "A single dose of the COVID-19 vaccine made by either Pfizer or AstraZeneca cuts a person’s risk of transmitting SARS-CoV-2 to their closest contacts by as much as half, according to an analysis of more than 365,000 households in the United Kingdom.", "Facebook.com", "missing"],["COVID research: a year of scientific milestones", "A single dose of the COVID-19 vaccine made by either Pfizer or AstraZeneca cuts a person’s risk of transmitting SARS-CoV-2 to their closest contacts by as much as half, according to an analysis of more than 365,000 households in the United Kingdom.", "Facebook.com", "missing"],["COVID research: a year of scientific milestones", "A single dose of the COVID-19 vaccine made by either Pfizer or AstraZeneca cuts a person’s risk of transmitting SARS-CoV-2 to their closest contacts by as much as half, according to an analysis of more than 365,000 households in the United Kingdom.", "Facebook.com", "missing"]])
num1 = np.array([["We love Coivd Cow Piss", "Drinking COW PISS = no COVID", "Whatsapp", 'missing'], ["We love Coivd Cow Piss", "Drinking COW PISS = no COVID", "Whatsapp", 'missing'], ["We love Coivd Cow Piss", "Drinking COW PISS = no COVID", "Whatsapp", 'missing'], ["We love Coivd Cow Piss", "Drinking COW PISS = no COVID", "Whatsapp", 'missing']])
X_predict = pd.DataFrame(num, columns=['title', 'text', 'source', 'label'])
X_p = new_data(X_predict)

X_p = df.drop(['title', 'text', 'source', 'label'], axis = 1)

print(cross_val_predict(svc, X_p, y))